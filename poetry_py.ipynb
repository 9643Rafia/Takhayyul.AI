{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>GRU MODEL TRAINING</h1>\n"
      ],
      "metadata": {
        "id": "FoCs2Kjvy76U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIC_qmphd8kQ",
        "outputId": "d152249f-375d-4f8b-8abf-2416df67b6b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 14ms/step - accuracy: 0.0564 - loss: 6.9645 - val_accuracy: 0.0343 - val_loss: 6.9409\n",
            "Epoch 2/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 13ms/step - accuracy: 0.0589 - loss: 6.7844 - val_accuracy: 0.0343 - val_loss: 6.9843\n",
            "Epoch 3/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 13ms/step - accuracy: 0.0579 - loss: 6.7722 - val_accuracy: 0.0343 - val_loss: 6.9635\n",
            "Epoch 4/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 13ms/step - accuracy: 0.0577 - loss: 6.7130 - val_accuracy: 0.0343 - val_loss: 6.9399\n",
            "Epoch 5/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 13ms/step - accuracy: 0.0622 - loss: 6.6849 - val_accuracy: 0.0415 - val_loss: 6.9277\n",
            "Epoch 6/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 13ms/step - accuracy: 0.0625 - loss: 6.6214 - val_accuracy: 0.0410 - val_loss: 6.9143\n",
            "Epoch 7/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 13ms/step - accuracy: 0.0636 - loss: 6.6034 - val_accuracy: 0.0425 - val_loss: 6.8921\n",
            "Epoch 8/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 13ms/step - accuracy: 0.0628 - loss: 6.5898 - val_accuracy: 0.0434 - val_loss: 6.8826\n",
            "Epoch 9/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 14ms/step - accuracy: 0.0647 - loss: 6.5481 - val_accuracy: 0.0430 - val_loss: 6.8856\n",
            "Epoch 10/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14ms/step - accuracy: 0.0649 - loss: 6.5302 - val_accuracy: 0.0416 - val_loss: 6.9212\n",
            "Epoch 11/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 13ms/step - accuracy: 0.0639 - loss: 6.5251 - val_accuracy: 0.0434 - val_loss: 6.8995\n",
            "Epoch 12/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 14ms/step - accuracy: 0.0652 - loss: 6.5090 - val_accuracy: 0.0416 - val_loss: 6.9049\n",
            "Epoch 13/100\n",
            "\u001b[1m5125/5125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 13ms/step - accuracy: 0.0654 - loss: 6.4963 - val_accuracy: 0.0426 - val_loss: 6.8933\n",
            "Training complete. Model and tokenizer saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import pickle\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = 'Roman-Urdu-Poetry.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Data Cleaning\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)      # Remove extra spaces\n",
        "    return text.strip().lower()\n",
        "\n",
        "poems = df['Poetry'].astype(str).values\n",
        "cleaned_poems = [clean_text(poem) for poem in poems]\n",
        "\n",
        "# Tokenization with a larger vocabulary\n",
        "max_vocab_size = 10000  # Increased vocab size\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(cleaned_poems)\n",
        "total_words = min(len(tokenizer.word_index) + 1, max_vocab_size)\n",
        "\n",
        "# Load Pre-trained Urdu Embeddings (FastText)\n",
        "#urdu_vectors = api.load(\"fasttext-wiki-news-subwords-300\")  # Loads pre-trained Urdu embeddings\n",
        "#embedding_dim = 300  # Use 300-dimensional embeddings\n",
        "\n",
        "# Create Embedding Matrix\n",
        "embedding_matrix = np.zeros((total_words, embedding_dim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index < total_words:\n",
        "        try:\n",
        "            embedding_matrix[index] = urdu_vectors[word]\n",
        "        except KeyError:\n",
        "            embedding_matrix[index] = np.random.normal(scale=0.6, size=(embedding_dim,))  # Random vector for unknown words\n",
        "\n",
        "# Create sequences with meaningful context\n",
        "input_sequences = []\n",
        "for poem in cleaned_poems:\n",
        "    token_list = tokenizer.texts_to_sequences([poem])[0]\n",
        "    for i in range(3, len(token_list)):  # Use larger training sequences\n",
        "        input_sequences.append(token_list[:i+1])\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = min(max([len(x) for x in input_sequences]), 50)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Create predictors and target\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# Define the Model with Pre-trained Urdu Embeddings\n",
        "def create_model(total_words, max_sequence_len, embedding_matrix):\n",
        "    model = Sequential([\n",
        "        Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_len-1, trainable=False),  # Use pre-trained embeddings\n",
        "        GRU(256, return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        GRU(128, return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        GRU(64),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(total_words, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                 optimizer='adam',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and Train the Model\n",
        "model = create_model(total_words, max_sequence_len, embedding_matrix)\n",
        "history = model.fit(\n",
        "    X, y,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save Model and Tokenizer\n",
        "model.save('urdu_poetry_model.keras')\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('max_sequence_len.pickle', 'wb') as handle:\n",
        "    pickle.dump(max_sequence_len, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"Training complete. Model and tokenizer saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model = load_model('urdu_poetry_model.keras')\n",
        "\n",
        "# Load tokenizer\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Load max_sequence_len\n",
        "with open('max_sequence_len.pickle', 'rb') as handle:\n",
        "    max_sequence_len = pickle.load(handle)\n",
        "\n",
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Applies temperature sampling to predicted probabilities.\n",
        "    Lower temperature → more deterministic, Higher temperature → more creative.\n",
        "    \"\"\"\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds + 1e-8) / temperature  # Apply temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)  # Normalize probabilities\n",
        "    return np.random.choice(len(preds), p=preds)  # Sample word index\n",
        "\n",
        "def generate_poem(seed_text, next_words, model, max_sequence_len, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generates a poem using temperature-based sampling.\n",
        "    \"\"\"\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted = sample_with_temperature(predicted_probs, temperature)  # Use temperature sampling\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        if output_word:\n",
        "            seed_text += \" \" + output_word  # Add new word to seed\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "def generate_formatted_poem(seed_text, model, max_sequence_len, num_lines=4, words_per_line=6, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generates a multi-line formatted poem.\n",
        "    \"\"\"\n",
        "    poem = []\n",
        "    current_line = seed_text\n",
        "\n",
        "    for line in range(num_lines):\n",
        "        if line > 0:\n",
        "            seed_words = current_line.split()[-3:]  # Keep last 3 words for context\n",
        "            current_line = \" \".join(seed_words)\n",
        "\n",
        "        current_line = generate_poem(current_line, words_per_line, model, max_sequence_len, temperature)\n",
        "        poem.append(current_line.strip())\n",
        "\n",
        "    return \"\\n\".join(poem)\n",
        "\n",
        "# Function to generate a poem\n",
        "def generate_urdu_poem(input_text, temperature=0.8):\n",
        "    try:\n",
        "        formatted_poem = generate_formatted_poem(\n",
        "            seed_text=input_text,\n",
        "            model=model,\n",
        "            max_sequence_len=max_sequence_len,\n",
        "            num_lines=4,\n",
        "            words_per_line=6,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return formatted_poem\n",
        "    except Exception as e:\n",
        "        return f\"Error generating poem: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        input_text = input(\"Enter a Roman Urdu word or phrase (or 'quit' to exit): \")\n",
        "        if input_text.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        temperature = 0.9\n",
        "\n",
        "        generated_poem = generate_urdu_poem(input_text, temperature)\n",
        "        print(\"\\nGenerated Poem:\")\n",
        "        print(\"--------------\")\n",
        "        print(generated_poem)\n",
        "        print(\"--------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XN9uL1crBBT",
        "outputId": "c699a7dc-7eb0-4375-967d-04a96fdf3cd6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a Roman Urdu word or phrase (or 'quit' to exit): rafia\n",
            "\n",
            "Generated Poem:\n",
            "--------------\n",
            "rafia kī dil us ke ik to\n",
            "ke ik to rahe e lab meñ pe ḳhauf\n",
            "meñ pe ḳhauf ki aahū kā liye e āv\n",
            "liye e āv shigāf bhī āḳhir tirā e yā\n",
            "--------------\n",
            "\n",
            "Enter a Roman Urdu word or phrase (or 'quit' to exit): habiba\n",
            "\n",
            "Generated Poem:\n",
            "--------------\n",
            "habiba e tiir tarāshā vahīñ anī ḳhud\n",
            "vahīñ anī ḳhud dil ho panja aate maiñ kahīñ\n",
            "aate maiñ kahīñ egā ki bātnī use bhī hai\n",
            "use bhī hai juuñ ga e haiñ kyā ki\n",
            "--------------\n",
            "\n",
            "Enter a Roman Urdu word or phrase (or 'quit' to exit): hateem\n",
            "\n",
            "Generated Poem:\n",
            "--------------\n",
            "hateem nahīñ ke mirī kam asad sho\n",
            "kam asad sho ik āvāz baithe huā zom pahlī\n",
            "huā zom pahlī ne haiñ to jañgal e hai\n",
            "jañgal e hai iz e kahīñ raat haiñ zindāñ\n",
            "--------------\n",
            "\n",
            "Enter a Roman Urdu word or phrase (or 'quit' to exit): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CRLvZyCly6J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "# Load Pre-trained Urdu Embeddings (FastText)\n",
        "urdu_vectors = api.load(\"word2vec-google-news-300\")  # Loads pre-trained Urdu embeddings\n",
        "embedding_dim = 300  # Use 300-dimensional embeddings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKwxY3ndkhqB",
        "outputId": "f8b441fe-7671-4090-fbfa-9ef81f3f776e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit\n"
      ],
      "metadata": {
        "id": "I1jtrbuLuKJ_",
        "outputId": "e41301ce-5f1b-46b6-beb6-3b4d5b65b3cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.25.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.42.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.42.0 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.serialization.load_weights_only = True"
      ],
      "metadata": {
        "id": "dDomKTcvugve"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "Me5UoR4FumCM",
        "outputId": "7eeb6b1c-9e3c-4f4d-ba08-4b78b0c0c010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.16.203.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "_28hVa80um3f",
        "outputId": "0cfecb96-b3dd-4e94-9eca-4f94b4266910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.16.203.38:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0Kyour url is: https://lovely-dragons-grin.loca.lt\n",
            "2025-02-07 19:00:41.644351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738954841.668356   19942 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738954841.675436   19942 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-07 19:00:45.221682: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1738954845.221851   19942 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13562 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "I0000 00:00:1738954865.141457   19973 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "2025-02-07 19:01:07.584 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-02-07 19:01:33.730 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-02-07 19:01:42.100 Uncaught app execution\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 121, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 591, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 8, in <module>\n",
            "    model = load_model('urdu_poetry_model.keras')\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\", line 189, in load_model\n",
            "    return saving_lib.load_model(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\", line 367, in load_model\n",
            "    return _load_model_from_fileobj(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\", line 444, in _load_model_from_fileobj\n",
            "    model = _model_from_config(\n",
            "            ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\", line 433, in _model_from_config\n",
            "    model = deserialize_keras_object(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\", line 718, in deserialize_keras_object\n",
            "    instance = cls.from_config(inner_config)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py\", line 355, in from_config\n",
            "    layer = serialization_lib.deserialize_keras_object(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\", line 730, in deserialize_keras_object\n",
            "    instance.build_from_config(build_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 455, in build_from_config\n",
            "    self.build(**config[\"shapes_dict\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 226, in build_wrapper\n",
            "    with obj._open_name_scope():\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\", line 664, in __exit__\n",
            "    super().__exit__(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/name_scope.py\", line 61, in __exit__\n",
            "    name_scope_stack.pop()\n",
            "    ^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'pop'\n",
            "2025-02-07 19:02:06.753 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-02-07 19:02:32.995 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model = load_model('urdu_poetry_model.keras')\n",
        "\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "with open('max_sequence_len.pickle', 'rb') as handle:\n",
        "    max_sequence_len = pickle.load(handle)\n",
        "\n",
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds + 1e-8) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return np.random.choice(len(preds), p=preds)\n",
        "\n",
        "def generate_poem(seed_text, next_words, model, max_sequence_len, temperature=0.8):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted = sample_with_temperature(predicted_probs, temperature)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        if output_word:\n",
        "            seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "def generate_formatted_poem(seed_text, model, max_sequence_len, num_lines=4, words_per_line=6, temperature=0.8):\n",
        "    poem = []\n",
        "    current_line = seed_text\n",
        "    for line in range(num_lines):\n",
        "        if line > 0:\n",
        "            seed_words = current_line.split()[-3:]\n",
        "            current_line = \" \".join(seed_words)\n",
        "        current_line = generate_poem(current_line, words_per_line, model, max_sequence_len, temperature)\n",
        "        poem.append(current_line.strip())\n",
        "    return \"\\n\".join(poem)\n",
        "\n",
        "# Streamlit UI\n",
        "st.set_page_config(page_title=\"Qalam-e-Roman (قلمِ رومن): Urdu Poetry Generator\", page_icon=\"🌸\", layout=\"centered\")\n",
        "st.title(\"🌸 Qalam-e-Roman (قلمِ رومن): Urdu Poetry Generator 🌸\")\n",
        "st.markdown(\"A beautifully designed interactive app to generate poetic verses in Roman Urdu.\")\n",
        "\n",
        "seed_text = st.text_input(\"Enter a Roman Urdu phrase to inspire your poem:\", \"\")\n",
        "temperature = 0.8\n",
        "\n",
        "generate_button = st.button(\"Takhleeq ✨\")\n",
        "\n",
        "if generate_button and seed_text:\n",
        "    generated_poem = generate_formatted_poem(seed_text, model, max_sequence_len, temperature=temperature)\n",
        "    st.subheader(\"🎶 Takhleeq-e-Ashaar : تخلیقِ اشعار 🎶\")\n",
        "    st.text_area(\"\", generated_poem, height=200)\n",
        "\n",
        "st.markdown(\"*Crafted with ❤️ for poetry lovers.*\")\n"
      ],
      "metadata": {
        "id": "B2NSPvcbvWrF",
        "outputId": "aa35d0ef-6349-4800-e330-5a58172c9542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    }
  ]
}